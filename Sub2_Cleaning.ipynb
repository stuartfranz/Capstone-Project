{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Import Data From Scrape\n",
    "\n",
    "reddit_data = pd.read_csv('./reddit_raw_scrape.csv')\n",
    "\n",
    "# Remove all Entries where there is no text\n",
    "\n",
    "del_index = reddit_data[reddit_data.selftext == '[removed]'].index\n",
    "reddit_data = reddit_data.drop(index=del_index).reset_index(drop=True)\n",
    "\n",
    "# Drop Columns with no target available\n",
    "\n",
    "reddit_data.dropna(subset = ['selftext','Ass','Ass2'],inplace=True)\n",
    "\n",
    "# Clean Text so all in same format\n",
    "\n",
    "reddit_data.selftext = [i.replace('\\n\\n',' ').replace('\\n',' ').replace(',','').replace('&amp;#x200B','') for i in reddit_data.selftext]\n",
    "\n",
    "# Convert text to Tuples after reading data changed the type\n",
    "\n",
    "reddit_data.Ass = reddit_data.Ass.map(eval)\n",
    "reddit_data.Ass2 = reddit_data.Ass2.map(eval)\n",
    "\n",
    "# Assign a target Class for each post from percentages of votes\n",
    "\n",
    "reddit_data['y'] = [i[0] for i in reddit_data.Ass]\n",
    "\n",
    "reddit_data.dropna(inplace=True,subset=['y'])\n",
    "\n",
    "reddit_data.reset_index(drop=True,inplace=True)\n",
    "\n",
    "# Drop columns where more than 10% of data is missing\n",
    "\n",
    "for i in reddit_data.columns:\n",
    "    if (reddit_data.isna().sum()/reddit_data.shape[0])[i]>=0.1:\n",
    "        reddit_data.drop(columns=[i],inplace=True)\n",
    "        \n",
    "# Drop columns that are either all one value or are clearly not useful\n",
    "        \n",
    "reddit_data.drop(columns=['author_flair_richtext','author','author_fullname','full_link','id','permalink',\n",
    "                          'subreddit_subscribers','thumbnail','title','url','whitelist_status',],inplace=True)\n",
    "\n",
    "for i in reddit_data.columns:\n",
    "    if len(reddit_data[i].value_counts()) == 1:\n",
    "        reddit_data.drop(columns=[i],inplace=True)\n",
    "        \n",
    "reddit_data.dropna(inplace=True)\n",
    "\n",
    "# collate guildings into one number\n",
    "\n",
    "reddit_data.gildings = [np.sum(list(i.values())) for i in reddit_data.gildings.map(eval).values]\n",
    "\n",
    "reddit_data = reddit_data.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_data.to_json('Clean_Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Next Section](./Sub3_EDA.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
